<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Piyush" />
  <title>Final Assignment_PracticalML</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Final Assignment_PracticalML</h1>
<p class="author">Piyush</p>
<p class="date"><code>r Sys.Date()</code></p>
</header>
<p><code>{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE)</code></p>
<h2 id="introduction">Introduction</h2>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har</p>
<p>The training and test data for this project are available in this two url’s:</p>
<p>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</p>
<p>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</p>
<h2 id="data-loading-and-processing">Data Loading and Processing</h2>
<p>```{r Data Loading and Processing, echo=TRUE}</p>
<p>set.seed(123) library(dplyr) library(caret) library(rpart) library(knitr) library(lattice) library(ggplot2) library(randomForest) library(data.table) library(gbm) library(rattle) library(rpart.plot) library(RColorBrewer) library(cowplot)</p>
<p>totalset&lt;-read.csv(“~/R files/pml-training.csv”,header = TRUE) testset&lt;-read.csv(“~/R files/pml-testing.csv”,header = TRUE)</p>
<pre><code>
## Cleaning and Exploratory Data Analysis

```{r Cleaning and Exploratory Data Analysis, echo=TRUE}

#Convert empty values with NA
totalset[totalset == &quot;&quot;] &lt;- NA
testset[testset == &quot;&quot;] &lt;- NA

#Now we will be removing those columns where most values are NA
totalset&lt;-totalset[, which(colMeans(!is.na(totalset)) &gt; 0.5)]
testset&lt;-testset[, which(colMeans(!is.na(testset)) &gt; 0.5)]

#Now we will remove initial ID and identification variables from the set

totalset &lt;- totalset[ , -(1:6)]
testset  &lt;- testset [ , -(1:6)]
</code></pre>
<h2 id="train-validation-split">Train-Validation Split</h2>
<p>```{r Train-Validation Split, echo=TRUE}</p>
<p>datapartition&lt;-createDataPartition(totalset$classe,p=0.75,list = FALSE) trainingset&lt;-totalset[datapartition,] cvset&lt;-totalset[-datapartition,]</p>
<pre><code>
## Model Testing

Here, we will try different models and will select the best one based on the its accuracy on Validation set.

## Decision Tree

```{r Decision Tree, echo=TRUE}

#First we will try simple decison tree

DT_Model&lt;-rpart(classe ~ .,data=trainingset,method=&quot;class&quot;)
DT_outofsample&lt;-predict(DT_Model,trainingset,type = &#39;class&#39;)
confusionMatrix(table(DT_outofsample, trainingset$classe))

DT_predict&lt;-predict(DT_Model,cvset,type = &#39;class&#39;)
confusionMatrix(table(DT_predict, cvset$classe))

</code></pre>
<p>So we are getting around 76.5% accuracy on the trainingset and 75% after testing on Validation set from this simple decision tree.</p>
<h2 id="now-we-will-try-to-form-the-random-forest-model">Now we will try to form the Random forest model</h2>
<p>```{r Random Forest, echo=TRUE}</p>
<p>set.seed(1967) rf_Model&lt;-randomForest(as.factor(classe) ~ ., data = trainingset,proximity=TRUE) rf_Model</p>
<pre><code>
Now we will plot the error data vs the number of trees

```{r, echo=TRUE}

oob.error.data&lt;-data.frame(Trees=rep(1:nrow(rf_Model$err.rate),times=1),Type=rep(c(&quot;Total error&quot;),each=nrow(rf_Model$err.rate)),Error=c(rf_Model$err.rate[,&quot;OOB&quot;]))
                 
ggplot(data=oob.error.data,aes(x=Trees,y=Error))+geom_line(aes(color=Type))          
</code></pre>
<p>So as we can see that after total no. of trees crossed 100, there is not any significant change in total error so we can set optimal number of trees at 100.</p>
<p>Now we can experiment with different number of splits to be considered at each node.</p>
<p>```{r, echo=TRUE}</p>
<p>rftrial&lt;-vector(length = 12)</p>
<p>for (i in 4:15){ tempmodel&lt;-randomForest(as.factor(classe) ~ ., data = trainingset,mtry=i,ntree=100) rftrial[i]&lt;-tempmodel<span class="math inline"><em>e</em><em>r</em><em>r</em>.<em>r</em><em>a</em><em>t</em><em>e</em>[<em>n</em><em>r</em><em>o</em><em>w</em>(<em>t</em><em>e</em><em>m</em><em>p</em><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></span>err.rate),1] }</p>
<p>rftrial</p>
<pre><code>

So we can note that keeping no. of splits at 10 will be our best solution
So finally, we will select this combination of ntree=100 and mtry=10 and will test this model on our cvset.

## Random Forest Final Model
```{r, echo=TRUE}

rf_finalModel&lt;-randomForest(as.factor(classe) ~ ., data = trainingset,mtry=10,ntree=100,proximity=TRUE)

predict_trainingdata&lt;-predict(rf_finalModel, newdata = trainingset)
confusionMatrix(table(predict_trainingdata, trainingset$classe))


predict_RF &lt;- predict(rf_finalModel, newdata = cvset)
confusionMatrix(table(predict_RF, cvset$classe))</code></pre>
<p>So we are getting 100% accuracy on the trainingdata and 99.9% accuracy with this model on our CVset which is really good.So, we will finalise this as our final model.</p>
<h2 id="prediction-on-testing-set">Prediction on Testing Set</h2>
<p>Now, we will use this model to get the prediction on our test data.</p>
<p>```{r, echo=TRUE}</p>
<p>test_predict&lt;-predict(rf_finalModel, newdata = testset[,-54]) test_predict</p>
<p>```</p>
</body>
</html>
